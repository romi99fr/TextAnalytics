{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "6n9wf1Cyhkz0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(42)\n",
        "\n",
        "from tensorflow.random import set_seed\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Tweets\n",
        "!curl -O https://raw.githubusercontent.com/carlosep93/Text-classification-with-Keras-lab/master/data/airline/tweets.txt\n",
        "\n",
        "#Download Labels\n",
        "!curl -O https://raw.githubusercontent.com/carlosep93/Text-classification-with-Keras-lab/master/data/airline/labels.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oj7RM-erhwaM",
        "outputId": "ecdb159c-031e-4897-c0fc-4a5e71fbba43"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1508k  100 1508k    0     0  6365k      0 --:--:-- --:--:-- --:--:-- 6365k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  125k  100  125k    0     0   682k      0 --:--:-- --:--:-- --:--:--  682k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(file_name):\n",
        "  raw_text = []\n",
        "  with open(file_name,'rb') as f:\n",
        "    return [str(line.strip()) for line in f.readlines()]"
      ],
      "metadata": {
        "id": "FdNrrH2nhx60"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = read_file('tweets.txt')\n",
        "\n",
        "labels = read_file('labels.txt')"
      ],
      "metadata": {
        "id": "EovDiPdCh0BG"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tweets_train, tweets_test, labels_train, labels_test = train_test_split(tweets, \n",
        "                                                                        labels, \n",
        "                                                                        test_size=0.33, \n",
        "                                                                        random_state=42)\n",
        "\n",
        "tweets_train, tweets_valid, labels_train, labels_valid = train_test_split(tweets_train, \n",
        "                                                                        labels_train, \n",
        "                                                                        test_size=0.33, \n",
        "                                                                        random_state=42)"
      ],
      "metadata": {
        "id": "l638DPVYh_3K"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_dict = {l:i for i,l in enumerate(set(labels_train))}\n",
        "print(label_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-lWALWSiCHU",
        "outputId": "a4447bce-b3f8-4d0a-9887-f7487192d838"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"b'positive'\": 0, \"b'neutral'\": 1, \"b'negative'\": 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_labels(split, label_dict=label_dict):\n",
        "  prep_labels = []\n",
        "  for label in split:\n",
        "    idx = label_dict[label]\n",
        "    classes = [0]*len(label_dict)\n",
        "    classes[idx] = 1\n",
        "    prep_labels.append(classes)\n",
        "  return np.asarray(prep_labels)"
      ],
      "metadata": {
        "id": "lSl3fGA7iDfh"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prep_labels_train = prepare_labels(labels_train, label_dict)\n",
        "prep_labels_valid = prepare_labels(labels_valid, label_dict)\n",
        "prep_labels_test = prepare_labels(labels_test, label_dict)"
      ],
      "metadata": {
        "id": "6JQ12wv_iFVp"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prep_labels_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpq7Aad8iGf3",
        "outputId": "25e9e76e-1049-48a9-c9fd-e28c0bb81aa8"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "stop_words = list(stopwords.words('english')) #About 150 stopwords\n",
        "stemmer = PorterStemmer()\n",
        "tknzr = TweetTokenizer()\n",
        "\n",
        "def clean_data(split):\n",
        "  clean_split = []\n",
        "  for sentence in split:\n",
        "    #Remove urls \n",
        "    sentence = re.sub(r'http\\S+', '', sentence)\n",
        "    #Tokenize\n",
        "    sentence = tknzr.tokenize(sentence)\n",
        "    #Lower Casing & Remove Stop Words\n",
        "    sentence = [word.lower() for word in sentence \n",
        "                  if word not in stop_words \n",
        "                  or not word.isalnum()]\n",
        "    #Stemming\n",
        "    sentence = ' '.join([stemmer.stem(word) for word in sentence])\n",
        "    \n",
        "    clean_split.append(sentence)\n",
        "\n",
        "  return clean_split"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNQYE_2niKu5",
        "outputId": "ee0b1c49-7f4e-4252-b96b-62b1bd865bf9"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_tweets_train = clean_data(tweets_train)\n",
        "clean_tweets_valid = clean_data(tweets_valid)\n",
        "clean_tweets_test = clean_data(tweets_test)"
      ],
      "metadata": {
        "id": "k__bkrEViMFe"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_tweets_train[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8mTqWuysiNUZ",
        "outputId": "550e8d47-e6af-416a-9bb1-6cba0304fe85"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"b ' @jetblu wish everyon felt like '\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_ds = tf.data.Dataset.from_tensor_slices((clean_tweets_train, prep_labels_train))\n",
        "raw_test_ds = tf.data.Dataset.from_tensor_slices((clean_tweets_test, prep_labels_test))\n",
        "raw_valid_ds = tf.data.Dataset.from_tensor_slices((clean_tweets_valid, prep_labels_valid))"
      ],
      "metadata": {
        "id": "veBya3kpiO17"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "max_features = 2000\n",
        "sequence_length = 20\n",
        "UNK = '<unk>'\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=2000,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "# Let's make a text-only dataset (no labels):\n",
        "text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "# Let's call `adapt`:\n",
        "vectorize_layer.adapt(text_ds)"
      ],
      "metadata": {
        "id": "tQX24yFniQXg"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tensor_label = tf.convert_to_tensor(label)\n",
        "    tensor_label = tf.reshape(tensor_label,[1,n_labels])\n",
        "    return vectorize_layer(text), tensor_label"
      ],
      "metadata": {
        "id": "KJlUgVSdiSDo"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_labels = len(label_dict)\n",
        "\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "valid_ds = raw_valid_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)"
      ],
      "metadata": {
        "id": "vtYo0d0TiTR4"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in test_ds:\n",
        "  print(batch)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLyIAA10iUeE",
        "outputId": "545bc369-e852-47c2-be6c-93f4fc4fd778"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(1, 20), dtype=int64, numpy=\n",
            "array([[  2,   9,  72, 385,   1, 157,  36,   1,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0]])>, <tf.Tensor: shape=(1, 3), dtype=int64, numpy=array([[1, 0, 0]])>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.backend import mean\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "embedding_dim = 10\n",
        "\n",
        "\n",
        "\n",
        "# Text input\n",
        "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "# 'embedding_dim'.\n",
        "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
        "x = mean(x,axis=1)\n",
        "\n",
        "# We project onto a single unit output layer:\n",
        "predictions = layers.Dense(n_labels, activation=\"softmax\", name=\"predictions\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, predictions)\n",
        "\n",
        "# Compile the model with crossentropy loss and an adam optimizer.\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer='Adam', metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "ZXPljjVLiWPd"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "batch_size = 32\n",
        "\n",
        "# Fit the model using the train and test datasets.\n",
        "history = model.fit(train_ds,\n",
        "          validation_data=valid_ds, \n",
        "          epochs=epochs, \n",
        "          batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Uy-uX44iY6r",
        "outputId": "dd063a75-88c8-4dce-d08d-4493d92a4003"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6571/6571 [==============================] - 19s 3ms/step - loss: 0.7727 - accuracy: 0.6679 - val_loss: 0.6661 - val_accuracy: 0.7278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "model.evaluate(test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6owCEsLiaIE",
        "outputId": "8476a953-2beb-4d9c-835d-84c50a8d065e"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4832/4832 [==============================] - 7s 2ms/step - loss: 0.6409 - accuracy: 0.7490\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6408934593200684, 0.7489652037620544]"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 10\n",
        "kernel_size = 5\n",
        "\n",
        "# Text input\n",
        "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "# 'embedding_dim'.\n",
        "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
        "x = layers.Conv1D(50, kernel_size, strides=3)(x)\n",
        "x = layers.Conv1D(30, kernel_size, strides=1)(x)\n",
        "x = mean(x,axis=1)\n",
        "\n",
        "\n",
        "#lstm = tf.keras.layers.LSTM(embedding_dim)\n",
        "#x = lstm(x)\n",
        "\n",
        "# We project onto a single unit output layer:\n",
        "predictions = layers.Dense(n_labels, activation=\"softmax\", name=\"predictions\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, predictions)\n",
        "\n",
        "# Compile the model with crossentropy loss and an adam optimizer.\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer='Adam', metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "iBPWSihwicpy"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "batch_size = 32\n",
        "\n",
        "# Fit the model using the train and test datasets.\n",
        "history = model.fit(train_ds,\n",
        "          validation_data=valid_ds, \n",
        "          epochs=epochs, \n",
        "          batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63XrSX5Zid5S",
        "outputId": "4fb0c56e-e8ff-421c-9ba8-fac2506c8b90"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6571/6571 [==============================] - 23s 3ms/step - loss: 0.6618 - accuracy: 0.7267 - val_loss: 0.5701 - val_accuracy: 0.7627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "model.evaluate(test_ds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2yoWxngifAY",
        "outputId": "0ceaa039-9155-472e-fb99-f4f9a08d33af"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4832/4832 [==============================] - 10s 2ms/step - loss: 0.5374 - accuracy: 0.7885\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5374395847320557, 0.7884933948516846]"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 10\n",
        "\n",
        "# Text input\n",
        "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "# 'embedding_dim'.\n",
        "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
        "x = layers.Conv1D(50, 7, strides=3)(x)\n",
        "x = layers.LSTM(10)(x)\n",
        "\n",
        "# We project onto a single unit output layer:\n",
        "predictions = layers.Dense(n_labels, activation=\"softmax\", name=\"predictions\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, predictions)\n",
        "\n",
        "# Compile the model with crossentropy loss and an adam optimizer.\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer='Adam', metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "mZISCPIpihJx"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 2\n",
        "batch_size = 32\n",
        "\n",
        "# Fit the model using the train and test datasets.\n",
        "history = model.fit(train_ds,\n",
        "          validation_data=valid_ds, \n",
        "          epochs=epochs, \n",
        "          batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBV8n6C5iiVW",
        "outputId": "4b12d246-1387-4135-ef59-bf78a45150cf"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "6571/6571 [==============================] - 52s 7ms/step - loss: 0.6727 - accuracy: 0.7241 - val_loss: 0.5864 - val_accuracy: 0.7637\n",
            "Epoch 2/2\n",
            "6571/6571 [==============================] - 54s 8ms/step - loss: 0.4874 - accuracy: 0.8113 - val_loss: 0.5921 - val_accuracy: 0.7587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "model.evaluate(test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKceZJO_ijkw",
        "outputId": "989669d6-ae28-48b2-d80a-cf963d6be908"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4832/4832 [==============================] - 14s 3ms/step - loss: 0.5654 - accuracy: 0.7796\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5654414892196655, 0.779594361782074]"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tip:** You can increase the number of epochs to 5 to see more differences.\n",
        "\n",
        "\n",
        "**Pooling model**\n",
        " 1. Try different values of embedding_dim. What happens when the embedding size is smaller (e.g. 20)? And bigger (e.g. 200)? \n",
        "        \n",
        "      Smaller (5) -> Tal y como vamos haciendo más pequeño la variable embedding_dim el accuracy va disminuyendo poco a poco y el loss va aumentando.\n",
        "\n",
        "      Bigger (200) -> En cada modelo, el accuracy aumenta y el loss disminuye.\n",
        "\n",
        "**Conv model**\n",
        "\n",
        "  2. Kernel size defines how many words we use to compute local context. Try different values of this parameter to see how the local context affects the performance of the model.\n",
        "        \n",
        "        Al variar este parámetro, podemos observar cómo cambia la capacidad del modelo para comprender y contextualizar el texto. Tal y como vamos aumentando el tamaño del kernel, el modelo tiene una ventana de contexto aún más amplia, lo que le permite capturar relaciones más extensas en el texto y puede entender conexiones más largas.\n",
        "\n",
        "  3. Add a new Conv1 layer to the model, after the current one. What paramenters would you use?\n",
        "        \n",
        "        Se agrega una nueva capa Conv1D con 30 filtros y el mismo kernel_size que la capa Conv1D anterior, que es 5. El parámetro strides se establece en 1, lo que indica que la operación de convolución avanzará un paso a la vez a lo largo de la secuencia..\n",
        "\n",
        "**LSTM model**\n",
        "\n",
        "  4. Like the embeddings, the LSTM units define how much information our representations can encode. Try different values of units and the LSTM layer. How it affects the accuracy? And the training time?\n",
        "\n",
        "    1. Número de unidades LSTM:\n",
        "      - Aumentar el número de unidades LSTM mejora la capacidad del modelo para capturar patrones y dependencias más complejos. Esto tiene como resultado una mayor precisión en el modelo.\n",
        "      - Sin embargo, también implica más parámetros que entrenar, lo cual aumenta la complejidad del modelo y puede llegar a requerir más tiempo de entrenamiento.\n",
        "\n",
        "    2. Capa LSTM:\n",
        "      - Agregar múltiples capas LSTM proporciona al modelo una mayor profundidad y la capacidad de aprender representaciones más abstractas de los datos. Esto puede mejorar la precisión del modelo.\n",
        "      - No obstante, agregar más capas LSTM también aumenta la complejidad del modelo y puede incrementar significativamente el tiempo de entrenamiento. \n",
        "\n",
        "\n",
        "  5. This model is by far the one with the most parameters, and that leads to overfitting. Dropout is regularization technique that drops to 0 a percentaje of the values that a layer gets, reducing the amount of information the layer gets. (https://keras.io/api/layers/regularization_layers/dropout/). How would this technique affect hour network? Keep in mind these considerations:\n",
        "\n",
        "        El Dropout es una técnica de regularización que ayuda a reducir el sobreajuste en el modelo al \"apagar\" un porcentaje de las activaciones de una capa durante el entrenamiento. Esto impide que ciertas unidades dependan demasiado de otras y promueve una mayor independencia entre las unidades, lo que puede ayudar a mejorar la generalización del modelo.\n",
        "\n",
        "  * Where would you place the new layer and why?\n",
        "\n",
        "      La capa Dropout se suele colocar después de la capa LSTM. Esto se debe a que esta capa suele tener más parámetros y, por lo tanto, son más propensas al sobreajuste. Colocar la capa Dropout después de esta capa ayuda a regularizarla.\n",
        "\n",
        "  * How does the performance change with different amounts of droput?\n",
        "\n",
        "      Es recomendable experimentar con diferentes valores para encontrar el equilibrio adecuado entre regularización y rendimiento del modelo.\n",
        "\n",
        "  * You should augment the number of epochs (epochs parameter) to see the results.\n",
        "\n",
        "      Esto se debe a que el Dropout introduce un efecto de \"ruido\" durante el entrenamiento, lo que puede hacer que el modelo tarde más tiempo en converger. Aumentar el número de épocas permite al modelo ajustarse mejor y alcanzar una mayor precisión.\n"
      ],
      "metadata": {
        "id": "UnrwcYPqk7vk"
      }
    }
  ]
}